{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitjobpostingnlpconda3fb0dd44090345c5858ed733fad6da5f",
   "display_name": "Python 3.7.6 64-bit ('job-posting-nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Computing TF-IDF Vectors with Scikit-Learn\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "excerpt from __Data Science Bookcamp: Five Python Projects__ MEAP V04 livebook by Leonard Apeltsin"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "NB. The author has directed the reader to interacting with the newsgroups dataset (`fetch_20newsgroups` from `sklearn.datasets`). As this dataset is large, it is not pre-packaged with Scikit-Learn. You may wish to define a shell variable named `SCIKIT_LEARN_DATA` for your environment so that the dataset is in a known singular location."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(remove=('headers', 'footers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[&#39;alt.atheism&#39;, &#39;comp.graphics&#39;, &#39;comp.os.ms-windows.misc&#39;, &#39;comp.sys.ibm.pc.hardware&#39;, &#39;comp.sys.mac.hardware&#39;, &#39;comp.windows.x&#39;, &#39;misc.forsale&#39;, &#39;rec.autos&#39;, &#39;rec.motorcycles&#39;, &#39;rec.sport.baseball&#39;, &#39;rec.sport.hockey&#39;, &#39;sci.crypt&#39;, &#39;sci.electronics&#39;, &#39;sci.med&#39;, &#39;sci.space&#39;, &#39;soc.religion.christian&#39;, &#39;talk.politics.guns&#39;, &#39;talk.politics.mideast&#39;, &#39;talk.politics.misc&#39;, &#39;talk.religion.misc&#39;]\n"
    }
   ],
   "source": [
    "print(newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---\n\nI was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n"
    }
   ],
   "source": [
    "# Return 1st newsgroup posting\n",
    "print(f'---\\n\\n{newsgroups.data[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---\n\nThe post at index 0 first appeared in the &#39;rec.autos&#39; group.\n"
    }
   ],
   "source": [
    "# Return the newsgroup name associated with the posting\n",
    "origin = newsgroups.target_names[newsgroups.target[0]]\n",
    "print(f'---\\n\\nThe post at index 0 first appeared in the \\'{origin}\\' group.')"
   ]
  },
  {
   "source": [
    "NB. So far, nothing unexpected… car post content was sourced from the car discussions on usenet!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---\n\nOur dataset contains 11,314 newsgroup posts.\n"
    }
   ],
   "source": [
    "# Count the number of newsgroup posts\n",
    "dataset_size = len(newsgroups.data)\n",
    "print(f'---\\n\\nOur dataset contains {dataset_size:,} newsgroup posts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets move on to transforming input texts into TF vectors via the Scikit-Learn `CountVectorizer` class\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "  (0, 108644)\t4\n  (0, 110106)\t1\n  (0, 57577)\t2\n  (0, 24398)\t2\n  (0, 79534)\t1\n  (0, 100942)\t1\n  (0, 37154)\t1\n  (0, 45141)\t1\n  (0, 70570)\t1\n  (0, 78701)\t2\n  (0, 101084)\t4\n  (0, 32499)\t4\n  (0, 92157)\t1\n  (0, 100827)\t6\n  (0, 79461)\t1\n  (0, 39275)\t1\n  (0, 60326)\t2\n  (0, 42332)\t1\n  (0, 96432)\t1\n  (0, 67137)\t1\n  (0, 101732)\t1\n  (0, 27703)\t1\n  (0, 49871)\t2\n  (0, 65338)\t1\n  (0, 14106)\t1\n  :\t:\n  (11313, 55901)\t1\n  (11313, 93448)\t1\n  (11313, 97535)\t1\n  (11313, 93393)\t1\n  (11313, 109366)\t1\n  (11313, 102215)\t1\n  (11313, 29148)\t1\n  (11313, 26901)\t1\n  (11313, 94401)\t1\n  (11313, 89686)\t1\n  (11313, 80827)\t1\n  (11313, 72219)\t1\n  (11313, 32984)\t1\n  (11313, 82912)\t1\n  (11313, 99934)\t1\n  (11313, 96505)\t1\n  (11313, 72102)\t1\n  (11313, 32981)\t1\n  (11313, 82692)\t1\n  (11313, 101854)\t1\n  (11313, 66399)\t1\n  (11313, 63405)\t1\n  (11313, 61366)\t1\n  (11313, 7462)\t1\n  (11313, 109600)\t1\n"
    }
   ],
   "source": [
    "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "&lt;class &#39;scipy.sparse.csr.csr_matrix&#39;&gt;\n"
    }
   ],
   "source": [
    "# Suspense! What kind of data structure did Scikit-Learn `CountVectorized` yield?\n",
    "print(type(tf_matrix))"
   ]
  },
  {
   "source": [
    "NB. The matrix is a *Compressed Sparse Row* (CSR) SciPy object. By storing only non-zero elements, the CSR matrix is efficient in storage and memory usage. There are some nuances between SciPy CSR matrix and a NumPy array so that in order to reduce confusion, a conversion will be done on the object. This will allow a better comprehension on the similarities and differences between the two matrix representations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n"
    }
   ],
   "source": [
    "tf_np_matrix = tf_matrix.toarray()\n",
    "print(tf_np_matrix)\n",
    "# -> Yields an 2D NumPy array"
   ]
  },
  {
   "source": [
    "NB. So this is a sparse matrix, no surprises there…\n",
    "Each matrix element corresponds to the count of a word within a post, and each matrix row represents a post. The matrix columns represent individual words so the column count equals the vocabulary size of the dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---\n\nOur collection of 11,314 newsgroup posts contain a total of 114,751 unique words.\n"
    }
   ],
   "source": [
    "assert tf_np_matrix.shape == tf_matrix.shape\n",
    "num_posts, vocabulary_size = tf_np_matrix.shape\n",
    "print(f'---\\n\\nOur collection of {num_posts:,} newsgroup posts contain a total of '\n",
    "      f'{vocabulary_size:,} unique words.')"
   ]
  },
  {
   "source": [
    "NB. There are over 114k words, but most posts only hold a few dozen of them. You can measure the unique word count of a post at index `i` by counting the number of non-zero elements in row `tf_np_matrix[i]`. NumPy can easily count the non-zero indeces of the vector at `tf_np_matrix[i]` from the `np.flatnonzero` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---\n\nThe newsgroup in row 0 contains 64 unique words.\nThe actual word-counts map to the following column indeces:\n\n[ 14106  15549  22088  23323  24398  27703  29357  30093  30629  32194\n  32305  32499  37154  39275  42332  42333  43643  45089  45141  49871\n  49881  50165  54442  55453  57577  58321  58842  60116  60326  64083\n  65338  67137  67140  68931  69080  70570  72915  75280  78264  78701\n  79055  79461  79534  82759  84398  87690  89161  92157  93304  95225\n  96145  96432 100406 100827 100942 101084 101732 108644 109086 109254\n 109294 110106 112936 113262]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tf_vector = tf_np_matrix[0]\n",
    "non_zero_indeces = np.flatnonzero(tf_vector)\n",
    "num_unique_words = non_zero_indeces.size\n",
    "print(f'---\\n\\nThe newsgroup in row 0 contains {num_unique_words} unique words.\\n'\n",
    "      f'The actual word-counts map to the following column indeces:\\n\\n'\n",
    "      f'{non_zero_indeces}')"
   ]
  },
  {
   "source": [
    "NB. We have the index values for the 64 unique words. Mapping back to the word-values is done via the `CountVectorizer` method `get_feature_names()`. The method-call will return a list of words, and each index `i` will correspond to the `i`-ith word within that list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[&#39;60s&#39;, &#39;70s&#39;, &#39;addition&#39;, &#39;all&#39;, &#39;anyone&#39;, &#39;be&#39;, &#39;body&#39;, &#39;bricklin&#39;, &#39;bumper&#39;, &#39;called&#39;, &#39;can&#39;, &#39;car&#39;, &#39;could&#39;, &#39;day&#39;, &#39;door&#39;, &#39;doors&#39;, &#39;early&#39;, &#39;engine&#39;, &#39;enlighten&#39;, &#39;from&#39;, &#39;front&#39;, &#39;funky&#39;, &#39;have&#39;, &#39;history&#39;, &#39;if&#39;, &#39;in&#39;, &#39;info&#39;, &#39;is&#39;, &#39;it&#39;, &#39;know&#39;, &#39;late&#39;, &#39;looked&#39;, &#39;looking&#39;, &#39;made&#39;, &#39;mail&#39;, &#39;me&#39;, &#39;model&#39;, &#39;name&#39;, &#39;of&#39;, &#39;on&#39;, &#39;or&#39;, &#39;other&#39;, &#39;out&#39;, &#39;please&#39;, &#39;production&#39;, &#39;really&#39;, &#39;rest&#39;, &#39;saw&#39;, &#39;separate&#39;, &#39;small&#39;, &#39;specs&#39;, &#39;sports&#39;, &#39;tellme&#39;, &#39;the&#39;, &#39;there&#39;, &#39;this&#39;, &#39;to&#39;, &#39;was&#39;, &#39;were&#39;, &#39;whatever&#39;, &#39;where&#39;, &#39;wondering&#39;, &#39;years&#39;, &#39;you&#39;]\n"
    }
   ],
   "source": [
    "# Get list of words from the `CountVectorizer` method\n",
    "words = vectorizer.get_feature_names()\n",
    "# List comprehension to view our non-zero words\n",
    "unique_words = [words[i] for i in non_zero_indeces]\n",
    "print(unique_words)"
   ]
  },
  {
   "source": [
    "NB. You can also get these words by calling `inverse_transform(tf_vector)`. This method call will return all the words associated with the input TF vector (which is a NumPy matrix, from the above)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Activity: View _word_ mention counts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We have printed the words from `newsgroup.data[0]`, but some of these words are more frequent than others. Lets dig down to find the more frequent words along with the count of use for that word. Represent them in a Pandas table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "   Word  Count\n    the      6\n   this      4\n    was      4\n    car      4\n     if      2\n     is      2\n     it      2\n   from      2\n     on      2\n anyone      2\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'Word': unique_words, # our list\n",
    "        'Count': tf_vector[non_zero_indeces]} # non-zero indeces NumPy array into the the first row (REM. position `0`) of the 2D NumPy array\n",
    "\n",
    "df = pd.DataFrame(data).sort_values('Count', ascending = False)\n",
    "print(df[:10].to_string(index = False))"
   ]
  },
  {
   "source": [
    "NB. So we have a top ten, but the top four words are not interesting. Good for us is that `CountVectorizer` has a class to remove *stop words* which, although part of speech and written language, do not carry information for the \"science\". We will now re-init a stop-word aware vectorizer amd re-compute the TF matrix. We will also regenerate out `words` list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "assert tf_matrix.shape[1] < 114751 # number of unique words known to be in our newsgroup vocabulary\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "for common_word in ['the', 'this', 'was', 'if', 'it', 'on']:\n",
    "    assert common_word not in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---\n\nAfter stop-word deletion, 34 unique words remain.\nThe 10 most frequent words are:\n\n       Word  Count\n        car      4\n        60s      1\n        saw      1\n    looking      1\n       mail      1\n      model      1\n production      1\n     really      1\n       rest      1\n   separate      1\n"
    }
   ],
   "source": [
    "tf_np_matrix = tf_matrix.toarray()\n",
    "tf_vector = tf_np_matrix[0]\n",
    "non_zero_indices = np.flatnonzero(tf_vector)\n",
    "unique_words = [words[index] for index in non_zero_indices]\n",
    "data = {'Word': unique_words,\n",
    "        'Count': tf_vector[non_zero_indices]}\n",
    "\n",
    "df = pd.DataFrame(data).sort_values('Count', ascending=False)\n",
    "print(f'---\\n\\nAfter stop-word deletion, {df.shape[0]} unique words remain.')\n",
    "print('The 10 most frequent words are:\\n')\n",
    "print(df[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB. Each of the 34 words in our dataframe appears in a certain fraction of newsgroup posts. In NLP, this fraction is referred to as the _document frequency_ of a word. From here, the job of the scientist is the hypothesize that document frequencies can be used to improve word rankings and thus our analysis. Initially, we will limit the exploration to a single document. Later, we will generalize the insights we obtain to the other documents in the dataset.\n",
    "\n",
    "#### Interlude: Common Scikit-Learn CountVectorizer Methods\n",
    "`vectorizer = CountVectorizer()`: Initializes a `CountVectorizer` object capable of vectorizing input texts based on their TF counts.\n",
    "\n",
    "`vectorizer = CountVectorizer(stopwords='english')`: Initializes an object capable of vectorizing input texts, while filtering for common English words like \"this\" or \"the\".\n",
    "\n",
    "`tf_matrix = vectorizer.fit_transform(texts)`: Executes TF vectorization on a list of input texts, using the initialized `vectorizer` object. Returns CSR matrix of term-frequency values. Each matrix row `i` corresponds to `texts[i]`. Each matrix column `j` corresponds to the term-frequency of word `j`.\n",
    "\n",
    "`vocabulary_list = vectorizer.get_feature_names()`: Returns the vocabulary-list associated with the columns of a computed TF matrix. Each column `j` of the matrix corresponds to `vocabulary_list[j]`."
   ]
  }
 ]
}