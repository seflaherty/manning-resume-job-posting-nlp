{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitjobpostingnlpconda3fb0dd44090345c5858ed733fad6da5f",
   "display_name": "Python 3.7.6 64-bit ('job-posting-nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Computing TF-IDF Vectors with Scikit-Learn\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "excerpt from __Data Science Bookcamp: Five Python Projects__ MEAP V04 livebook by Leonard Apeltsin\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Sweeping parts of the explainer text in this notebook was from the liveLessons notebook.\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "NB. The author has directed the reader to interacting with the newsgroups dataset (`fetch_20newsgroups` from `sklearn.datasets`). As this dataset is large, it is not pre-packaged with Scikit-Learn. You may wish to define a shell variable named `SCIKIT_LEARN_DATA` for your environment so that the dataset is in a known singular location."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(remove=('headers', 'footers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nI was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "# Return 1st newsgroup posting\n",
    "print(f'---\\n\\n{newsgroups.data[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nThe post at index 0 first appeared in the 'rec.autos' group.\n"
     ]
    }
   ],
   "source": [
    "# Return the newsgroup name associated with the posting\n",
    "origin = newsgroups.target_names[newsgroups.target[0]]\n",
    "print(f'---\\n\\nThe post at index 0 first appeared in the \\'{origin}\\' group.')"
   ]
  },
  {
   "source": [
    "NB. So far, nothing unexpected… car post content was sourced from the car discussions on usenet!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nOur dataset contains 11,314 newsgroup posts.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of newsgroup posts\n",
    "dataset_size = len(newsgroups.data)\n",
    "print(f'---\\n\\nOur dataset contains {dataset_size:,} newsgroup posts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets move on to transforming input texts into TF vectors via the Scikit-Learn `CountVectorizer` class\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 108644)\t4\n  (0, 110106)\t1\n  (0, 57577)\t2\n  (0, 24398)\t2\n  (0, 79534)\t1\n  (0, 100942)\t1\n  (0, 37154)\t1\n  (0, 45141)\t1\n  (0, 70570)\t1\n  (0, 78701)\t2\n  (0, 101084)\t4\n  (0, 32499)\t4\n  (0, 92157)\t1\n  (0, 100827)\t6\n  (0, 79461)\t1\n  (0, 39275)\t1\n  (0, 60326)\t2\n  (0, 42332)\t1\n  (0, 96432)\t1\n  (0, 67137)\t1\n  (0, 101732)\t1\n  (0, 27703)\t1\n  (0, 49871)\t2\n  (0, 65338)\t1\n  (0, 14106)\t1\n  :\t:\n  (11313, 55901)\t1\n  (11313, 93448)\t1\n  (11313, 97535)\t1\n  (11313, 93393)\t1\n  (11313, 109366)\t1\n  (11313, 102215)\t1\n  (11313, 29148)\t1\n  (11313, 26901)\t1\n  (11313, 94401)\t1\n  (11313, 89686)\t1\n  (11313, 80827)\t1\n  (11313, 72219)\t1\n  (11313, 32984)\t1\n  (11313, 82912)\t1\n  (11313, 99934)\t1\n  (11313, 96505)\t1\n  (11313, 72102)\t1\n  (11313, 32981)\t1\n  (11313, 82692)\t1\n  (11313, 101854)\t1\n  (11313, 66399)\t1\n  (11313, 63405)\t1\n  (11313, 61366)\t1\n  (11313, 7462)\t1\n  (11313, 109600)\t1\n"
     ]
    }
   ],
   "source": [
    "# Do the transform\n",
    "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Suspense! What kind of data structure did Scikit-Learn `CountVectorized` yield?\n",
    "print(type(tf_matrix))"
   ]
  },
  {
   "source": [
    "NB. The matrix is a *Compressed Sparse Row* (CSR) SciPy object. By storing only non-zero elements, the CSR matrix is efficient in storage and memory usage. There are some nuances between SciPy CSR matrix and a NumPy array so that in order to reduce confusion, a conversion will be done on the object. This will allow a better comprehension on the similarities and differences between the two matrix representations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# For discussion, we are changing the CSR matrix to a NumPy ndarray\n",
    "tf_np_matrix = tf_matrix.toarray()\n",
    "print(tf_np_matrix)\n",
    "# -> Yields an 2D NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n(11314, 114441)\n(114441,)\n(11314,)\n"
     ]
    }
   ],
   "source": [
    "# For sure!\n",
    "print(type(tf_np_matrix))\n",
    "# print(tf_np_matrix.shape) # -> init is slow to count to (11314, 114441)\n",
    "print(tf_np_matrix[0,:].shape) # ???\n",
    "print(tf_np_matrix[:,0].shape) # REM. 11,314 is len(newsgroup.data)"
   ]
  },
  {
   "source": [
    "NB. So this is a sparse matrix, no surprises there…\n",
    "The printed matrix is a 2D NumPy array. Each matrix element corresponds to the count of a word within a post, and each matrix row represents a post. The matrix columns represent individual words so the column count equals the vocabulary size of the dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nOur collection of 11,314 newsgroup posts contain a total of 114,751 unique words.\n"
     ]
    }
   ],
   "source": [
    "# Get some stats\n",
    "assert tf_np_matrix.shape == tf_matrix.shape\n",
    "num_posts, vocabulary_size = tf_np_matrix.shape\n",
    "print(f'---\\n\\nOur collection of {num_posts:,} newsgroup posts contain a total of '\n",
    "      f'{vocabulary_size:,} unique words.')"
   ]
  },
  {
   "source": [
    "NB. There are over 114k words, but most posts only hold a few dozen of them. You can measure the unique word count of a post at index `i` by counting the number of non-zero elements in row `tf_np_matrix[i]`. NumPy can easily count the non-zero indices of the vector at `tf_np_matrix[i]` from the `np.flatnonzero` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nThe newsgroup in row 0 contains 34 unique words.\nThe actual word-counts map to the following column indices:\n\n[ 14106  15549  22085  29307  30041  30577  32139  32441  39212  42264\n  42265  43571  45010  45062  50060  55328  58709  63943  65197  66993\n  66996  68934  72762  84198  87487  88958  91953  93095  95006  95918\n  96205 100175 109803 112632]\n"
     ]
    }
   ],
   "source": [
    "# Do the 👆 task\n",
    "import numpy as np\n",
    "tf_vector = tf_np_matrix[0]\n",
    "non_zero_indices = np.flatnonzero(tf_vector) # Equal to `a.ravel().nonzero()[0]` where `a` is array_like\n",
    "num_unique_words = non_zero_indices.size\n",
    "print(f'---\\n\\nThe newsgroup in row 0 contains {num_unique_words} unique words.\\n'\n",
    "      f'The actual word-counts map to the following column indices:\\n\\n'\n",
    "      f'{non_zero_indices}')"
   ]
  },
  {
   "source": [
    "NB. We have the index values for the 64 unique words. Mapping back to the word-values is done via the `CountVectorizer` method `get_feature_names()`. The method-call will return a list of words, and each index `i` will correspond to the `i`-ith word within that list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['60s', '70s', 'addition', 'body', 'bricklin', 'bumper', 'called', 'car', 'day', 'door', 'doors', 'early', 'engine', 'enlighten', 'funky', 'history', 'info', 'know', 'late', 'looked', 'looking', 'mail', 'model', 'production', 'really', 'rest', 'saw', 'separate', 'small', 'specs', 'sports', 'tellme', 'wondering', 'years']\n"
     ]
    }
   ],
   "source": [
    "# Get list of words from the `CountVectorizer` method\n",
    "words = vectorizer.get_feature_names()\n",
    "# List comprehension to view our non-zero words\n",
    "unique_words = [words[i] for i in non_zero_indices]\n",
    "print(unique_words)"
   ]
  },
  {
   "source": [
    "NB. You can also get these words by calling `inverse_transform(tf_vector)`. This method call will return all the words associated with the input TF vector (which is a NumPy matrix, from the above)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Activity: View _word_ mention counts by extracting _Non-Zero_ elements of 1D NumPy arrays"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`non_zero_indices = np.flatnonzero(np_vector)`: Returns the non-zero indices in a 1D NumPy array.\n",
    "\n",
    "`non_zero_vector = np_vector[non_zero_indices]`: Selects the non-zero elements of a 1D NumPy array (assuming `non_zero_indices` corresponds to non-zero indices of that array).\n",
    "\n",
    "We have printed the words from `newsgroup.data[0]`, but some of these words are more frequent than others. Lets dig down to find the more frequent words along with the count of use for that word. Represent them in a Pandas table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       Word  Count\n        car      4\n        60s      1\n        saw      1\n    looking      1\n       mail      1\n      model      1\n production      1\n     really      1\n       rest      1\n   separate      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'Word': unique_words, # our list\n",
    "        'Count': tf_vector[non_zero_indices]} # non-zero indices NumPy array into the the first row (REM. position `0`) of the 2D NumPy array\n",
    "\n",
    "df = pd.DataFrame(data).sort_values('Count', ascending = False)\n",
    "print(df[:10].to_string(index = False))"
   ]
  },
  {
   "source": [
    "NB. So we have a top ten, but the top words are not interesting. Good for us is that `CountVectorizer` has a class to remove *stop words* which, although part of speech and written language, do not carry information for the \"science\". We will now re-init a stop-word aware vectorizer and re-compute the TF matrix. We will also regenerate our `Words` list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the transform and end with an assert that common stop words are dropped\n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "assert tf_matrix.shape[1] < 114751 # number of unique words known to be in our newsgroup vocabulary\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "for common_word in ['the', 'this', 'was', 'if', 'it', 'on']:\n",
    "    assert common_word not in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nAfter stop-word deletion, 34 unique words remain.\nThe 10 most frequent words are:\n\n       Word  Count\n        car      4\n        60s      1\n        saw      1\n    looking      1\n       mail      1\n      model      1\n production      1\n     really      1\n       rest      1\n   separate      1\n"
     ]
    }
   ],
   "source": [
    "tf_np_matrix = tf_matrix.toarray()\n",
    "tf_vector = tf_np_matrix[0]\n",
    "non_zero_indices = np.flatnonzero(tf_vector)\n",
    "unique_words = [words[index] for index in non_zero_indices]\n",
    "data = {'Word': unique_words,\n",
    "        'Count': tf_vector[non_zero_indices]}\n",
    "\n",
    "df = pd.DataFrame(data).sort_values('Count', ascending=False)\n",
    "print(f'---\\n\\nAfter stop-word deletion, {df.shape[0]} unique words remain.')\n",
    "print('The 10 most frequent words are:\\n')\n",
    "print(df[:10].to_string(index=False))"
   ]
  },
  {
   "source": [
    "### Activity: Ranking _words_ by both post-frequency and count"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "NB. Each of the 34 words in our dataframe appears in a certain fraction of newsgroup posts. In NLP, this fraction is referred to as the _document frequency_ of a word. From here, the job of the scientist is the hypothesize that document frequencies can be used to improve word rankings and thus our analysis. Initially, we will limit the exploration to a single document. Later, we will generalize the insights we obtain to the other documents in the dataset.\n",
    "\n",
    "#### Interlude: Common Scikit-Learn CountVectorizer Methods\n",
    "`vectorizer = CountVectorizer()`: Initializes a `CountVectorizer` object capable of vectorizing input texts based on their TF counts.\n",
    "\n",
    "`vectorizer = CountVectorizer(stopwords='english')`: Initializes an object capable of vectorizing input texts, while filtering for common English words like \"this\" or \"the\".\n",
    "\n",
    "`tf_matrix = vectorizer.fit_transform(texts)`: Executes TF vectorization on a list of input texts, using the initialized `vectorizer` object. Returns CSR matrix of term-frequency values. Each matrix row `i` corresponds to `texts[i]`. Each matrix column `j` corresponds to the term-frequency of word `j`.\n",
    "\n",
    "`vocabulary_list = vectorizer.get_feature_names()`: Returns the vocabulary-list associated with the columns of a computed TF matrix. Each column `j` of the matrix corresponds to `vocabulary_list[j]`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Begin an exploration to compute 34 document frequencies to try and improve our word relevancy rankings. We can compute these frequencies using a series of NumPy matrix manipulations. First, select thise columns of `tf_np_matrix` that correspond to the 34 non-zero indices within the `non_zero_indices` array. The sub-matrix is available via `tf_np_matrix[:, non_zero_indices]`. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nGet the sub-matrix corresponding to the 34 words within post 0.\nThe first row in the sub-matrix is:\n\n[1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "sub_matrix = tf_np_matrix[:, non_zero_indices]\n",
    "print(f'---\\n\\nGet the sub-matrix corresponding to the 34 words within post 0.'\n",
    "      f'\\nThe first row in the sub-matrix is:\\n\\n{sub_matrix[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n(11314, 34)\n384676\n"
     ]
    }
   ],
   "source": [
    "# (szf) For show\n",
    "print(type(sub_matrix))\n",
    "print(sub_matrix.shape)\n",
    "print(sub_matrix.size) # 11,314 * 34 -> 384,676"
   ]
  },
  {
   "source": [
    "NB. The first row of `sub_matrix` corresponds to the 34 word counts in `df`. Together, all the matrix rows correpsond to counts across all posts. However, the goal is to know whether a work is present or absent from each post. Consequently, we will need to convert the counts to binary values (a binary matrix, if you will). Then each element `(i, j)` shall equal 1 if word `i` is in post `j`. We binarize the sub-matrix ny importing `binarize` from `sklearn.preprocessing` and then sampling the results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1 1 1 ... 1 1 1]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 1 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "binary_matrix = binarize(sub_matrix)\n",
    "print(binary_matrix)"
   ]
  },
  {
   "source": [
    "Now add together the rows of our binary sub-matrix producing a vector of integer counts. Each `i`th vector element will equal the number of unique posts in which word `i` is present. Summation of the array need only `axis = 0` passed to the `sum` method of the array. \n",
    "NB. A 2D NumPy array contains two axes. Axis0 is horizontal rows and axis1 is vertical. The summation is a vector of summed columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nThis vector counts the unique posts in which each word is mentioned:\n[  18   21  202  314    4   26  802  536  842  154   67  348  184   25\n    7  368  469 3093  238  268  780  901  292   95 1493  407  354  158\n  574   95   98    2  295 1174]\n"
     ]
    }
   ],
   "source": [
    "# Sum of the unique words in the matrix\n",
    "unique_post_mentions = binary_matrix.sum(axis = 0)\n",
    "print(f'---\\n\\nThis vector counts the unique posts in which each word is mentioned:\\n'\n",
    "      f'{unique_post_mentions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "# No change in our sample\n",
    "print(unique_post_mentions.size)"
   ]
  },
  {
   "source": [
    "NB. We should note that the above three procedures can be combined into a single line of code, by running `binarize(tf_np_matrix[:,non_zero_indices]).sum(axis=0)`. Furthermore, substituting NumPy’s `tf_np_matrix` with SciPy’s `tf_matrix` will still produce the same post mention-counts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nNumPy matrix-generated counts:\n [  18   21  202  314    4   26  802  536  842  154   67  348  184   25\n    7  368  469 3093  238  268  780  901  292   95 1493  407  354  158\n  574   95   98    2  295 1174]\n\n---\n\nCSR matrix-generated counts:\n [[  18   21  202  314    4   26  802  536  842  154   67  348  184   25\n     7  368  469 3093  238  268  780  901  292   95 1493  407  354  158\n   574   95   98    2  295 1174]]\n"
     ]
    }
   ],
   "source": [
    "# Do over, just for the academic rub\n",
    "np_post_mentions = binarize(tf_np_matrix[:,non_zero_indices]).sum(axis=0)\n",
    "csr_post_mentions = binarize(tf_matrix[:,non_zero_indices]).sum(axis=0)\n",
    "print(f'---\\n\\nNumPy matrix-generated counts:\\n {np_post_mentions}\\n')\n",
    "print(f'---\\n\\nCSR matrix-generated counts:\\n {csr_post_mentions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n(34,)\n^ NumPy matrix\n\n---\n\nv CSR matrix\n<class 'numpy.matrix'>\n(1, 34)\n"
     ]
    }
   ],
   "source": [
    "# (szf) For show\n",
    "print(type(np_post_mentions))\n",
    "print(np_post_mentions.shape)\n",
    "print('^ NumPy matrix\\n\\n---\\n\\nv CSR matrix')\n",
    "print(type(csr_post_mentions))\n",
    "print(csr_post_mentions.shape)"
   ]
  },
  {
   "source": [
    "### Activity: Methods for Aggregating Matrix Rows"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`vector_of_sums = np_matrix.sum(axis=0)`: Sums-up the rows of a NumPy matrix. If `np_matrix` is a TF matrix, then vector_of_sums[i] equals the total mention-count of word `i` within the dataset.\n",
    "\n",
    "`vector_of_sums = binary( np_matrix).sum(axis=0)`: Converts a NumPy matrix to binary, and then sums-ups its rows. If `np_matrix` is a TF matrix, then `vector_of_sums[i]` equals the total count of texts in which word `i` is mentioned.\n",
    "\n",
    "`matrix_1D = binary( csf_matrix).sum(axis=0)`: Converts a CSR matrix to binary, and then sums-ups its rows. The returned result is a special 1-dimensional matrix object. _It is not a NumPy vector_. The `matrix_1D` can be converted into a NumPy vector by running `np.asarray(matrix_1D)[0]`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "New goal: transform the word counts into document frequencies and align these frequencies with `df.Word`. Afterwards, we'll output all the words that are mentioned in at-least 10% of newsgroup posts. If we (as scientists, remember) hypothesize that the printed words will not be specific to a particular topic. If the hypothesis is correct, then these words will not be very relevant."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Word  Count  Document Frequency\n   know      1            0.273378\n really      1            0.131960\n  years      1            0.103765\n"
     ]
    }
   ],
   "source": [
    "# Print the words with the highest document frequency\n",
    "document_frequencies = unique_post_mentions / dataset_size\n",
    "data = {'Word': unique_words,\n",
    "        'Count': tf_vector[non_zero_indices],\n",
    "        'Document Frequency': document_frequencies}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_common_words = df[df['Document Frequency'] >= .1]\n",
    "print(df_common_words.to_string(index=False))"
   ]
  },
  {
   "source": [
    "From the 34 unique words, three have a document frequency greater than 0.1. These words are very general and not specific to a usenet post on cars (esp. word \"really\"). Lets apply the discovered document frequencies for ranking purposes. Lets rank our words by relevance, in the following manner. First, we’ll sort the word by count, from greatest to smallest. Afterwards, all words with equal count will be sorted by document frequency, from smallest to greatest. In Pandas, we can execute this dual-column sorting by running `df.sort_values(['Count', 'Document Frequency'], ascending=[True, False])`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       Word  Count  Document Frequency          IDF  Combined\n        car      4            0.047375    21.108209  5.297806\n     tellme      1            0.000177  5657.000000  3.752586\n   bricklin      1            0.000354  2828.500000  3.451556\n      funky      1            0.000619  1616.285714  3.208518\n        60s      1            0.001591   628.555556  2.798344\n        70s      1            0.001856   538.761905  2.731397\n  enlighten      1            0.002210   452.560000  2.655676\n     bumper      1            0.002298   435.153846  2.638643\n      doors      1            0.005922   168.865672  2.227541\n production      1            0.008397   119.094737  2.075893\n"
     ]
    }
   ],
   "source": [
    "# Ranking words by count and frequency\n",
    "df_sorted = df.sort_values(['Count', 'Document Frequency'],\n",
    "                           ascending=[False, True])\n",
    "print(df_sorted[:10].to_string(index=False))"
   ]
  },
  {
   "source": [
    "So there are things of interest in this printout… the word 'bumper' is both car-related and in the resultset. With the two-level sorting, most of the resultset has a count of 1 and it shows that the run-in term \"tellme\" has the least document frequency from our `df.Word`.\n",
    "This can be simplifed for understanding by combining the word counts and the document frequencies into one score. One approach is to device each word-count by its associated document frequency. This means that the resulting value will go up if:\n",
    "* The word-count goes up\n",
    "* The document frequency goes down\n",
    "\n",
    "Start by computing `1/document_frequencies`, producing an array of inverse document frequencies (commonly shortened to IDF). Next, we’ll multiply `df.Word` by the IDF array,in order to compute the combined score. We’ll then add both the IDF values and our combined scores to our Pandas table. Finally, we’ll sort on the combined score at printout."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       Word  Count  Document Frequency          IDF     Combined\n     tellme      1            0.000177  5657.000000  5657.000000\n   bricklin      1            0.000354  2828.500000  2828.500000\n      funky      1            0.000619  1616.285714  1616.285714\n        60s      1            0.001591   628.555556   628.555556\n        70s      1            0.001856   538.761905   538.761905\n  enlighten      1            0.002210   452.560000   452.560000\n     bumper      1            0.002298   435.153846   435.153846\n      doors      1            0.005922   168.865672   168.865672\n      specs      1            0.008397   119.094737   119.094737\n production      1            0.008397   119.094737   119.094737\n"
     ]
    }
   ],
   "source": [
    "# Combining counts and frequencies into a single score\n",
    "inverse_document_frequencies = 1 / document_frequencies\n",
    "df['IDF'] = inverse_document_frequencies\n",
    "df['Combined'] = df.Count * inverse_document_frequencies\n",
    "df_sorted = df.sort_values('Combined', ascending=False)\n",
    "print(df_sorted[:10].to_string(index=False))"
   ]
  },
  {
   "source": [
    "💥 There is a problem now! The word *car* is no longer at the top of the list. /TBD. Look within the table/ The printout has some huge IDF values, but the word-count range is very small with values from 1 to 4. When we multiply word-counts by IDF values, the IDF will dominate. The counts (as for word \"car\" with 4 occurences) will then have no impact on the final results. \n",
    "\n",
    "This is a common problem for DataSci. One technique is to apply a logarithmic function. For example `np.log10(100000)` returns a value of `6` which is the count of zeroes in the value. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Lets recompute our ranking score by running `df.Count * np.log10(df.IDF)`. The product of the counts and the shrunken IDF values should lead to a more reasonable ranking metric."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      Word  Count  Document Frequency          IDF  Combined\n       car      4            0.047375    21.108209  5.297806\n    tellme      1            0.000177  5657.000000  3.752586\n  bricklin      1            0.000354  2828.500000  3.451556\n     funky      1            0.000619  1616.285714  3.208518\n       60s      1            0.001591   628.555556  2.798344\n       70s      1            0.001856   538.761905  2.731397\n enlighten      1            0.002210   452.560000  2.655676\n    bumper      1            0.002298   435.153846  2.638643\n     doors      1            0.005922   168.865672  2.227541\n     specs      1            0.008397   119.094737  2.075893\n"
     ]
    }
   ],
   "source": [
    "# Adjustment of combined score using logarithms\n",
    "df['Combined'] = df.Count * np.log10(df.IDF)\n",
    "df_sorted = df.sort_values('Combined', ascending=False)\n",
    "print(df_sorted[:10].to_string(index=False))"
   ]
  },
  {
   "source": [
    "We have clawed back words \"car\" and \"bumper\", whereas word \"really\" remains missing from the list.\n",
    "Our effective score is called the **term frequency-inverse document frequency**, or TFIDF for short. The TFIDF can be computed by taking the product of the TF (word-count) with the log of the IDF."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Mathematically, `np.log(1/x)` is equal to `-np.log(x)`. Therefore, we can compute the TFIDF directly from the document frequencies. We simply need to run `df.Count * -np.log10(document_frequences)`. Also please be aware that other, less common formulations of TFIDF exist in the literature. For instance, when dealing with large documents, some NLP practitioners compute the TFIDF as `np.log(df.Count + 1) * -np.log10(document_frequences)`. *This compute limits the influence of any very common word with a document.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For most real-world text datasets, TFIDF produces good ranking results. Furthermore, the metric has additional uses. It can be utilized to vectorize words within a document. The numeric content of `df.Combined` is essentially a vector. It was produced by modifying the TF vector stored in `df.Count`. In this same manner, we can transform any TF vector into a TFIDF vector. We just need to multiply the TF vector by the log of inverse document frequencies."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Within larger text datasets, the transform of TF vectors into more complicated TFIDF vectors will provide a greater signal of textual similarity and divergence. For example, two texts that are both discussing \"cars\" are more likely to cluster together if their irrelevant vector elements are penalized. Thus, penalizing common words using the IDF will improve the clustering of large text collections."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "NB. The transform from TF vectors to TFIDF vectors is not necessarily true of smaller datasets where the number of documents is low and the document frequency is high. The IDF might be too small to improve the clustering results meaningfully."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Activity: Computing TFIDF Vectors with Scikit-Learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The `TfidfVectorizer` class is nearly identical to `CountVectorizer`, except that it takes IDF into account during the vectorization process. Initializing the class with `stop_words` will yield an object parameterized to ignore all stop words. Upon performing `fit_transform(newsgroups.data)`, we have a matric of vectorized TFIDF values. The shape of the matrix will remain the same between this object and that of `tf_matrix`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(newsgroups.data)\n",
    "assert tfidf_matrix.shape == tf_matrix.shape, \"The matrices do not have the same shape.\""
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 75,
   "outputs": []
  },
  {
   "source": [
    "Our `tfdif_vectorizer` has learned the same vocabulary as the simpler TF vectorizer. In fact, the indices of words in `tfidf_matrix` are identical to those of `tf_matrix`. We can confirm this by calling `tfidf_vectorizer.get_feature_names()`. The method-call will return an ordered list of words that is identical to our previously computed words list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tfidf_vectorizer.get_feature_names() == words, \"The ordered lists are different.\""
   ]
  },
  {
   "source": [
    "Since word-order is preserved, we should expect the non-zero indices of `tfidf_matrix[0]` to equal our previously computed non_zero_indices array. We’ll confirm below, after converting `tfidf_matrix` from a CSR data-structure to a NumPy array."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_np_matrix = tfidf_matrix.toarray()\n",
    "tfidf_vector = tfidf_np_matrix[0]\n",
    "tfidf_non_zero_indices = np.flatnonzero(tfidf_vector)\n",
    "assert np.array_equal(tfidf_non_zero_indices,\n",
    "                      non_zero_indices), \"The NumPy arrays are different.\""
   ]
  },
  {
   "source": [
    "NB. The non-zero indices of `tf_vector` and `tfidif_vector` are identical! We thus can add the TFIDF vector as a column in our existing df table. Adding a TFIDF column will allow us to compare Scikit-Learn’s output with our manually-computed score."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding TFIDF vector to the existing Pandas table\n",
    "df['TFIDF'] = tfidf_vector[non_zero_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      Word  Count  Document Frequency          IDF  Combined     TFIDF\n       car      4            0.047375    21.108209  5.297806  0.459552\n    tellme      1            0.000177  5657.000000  3.752586  0.262118\n  bricklin      1            0.000354  2828.500000  3.451556  0.247619\n     funky      1            0.000619  1616.285714  3.208518  0.234280\n       60s      1            0.001591   628.555556  2.798344  0.209729\n       70s      1            0.001856   538.761905  2.731397  0.205568\n enlighten      1            0.002210   452.560000  2.655676  0.200827\n    bumper      1            0.002298   435.153846  2.638643  0.199756\n     doors      1            0.005922   168.865672  2.227541  0.173540\n     specs      1            0.008397   119.094737  2.075893  0.163752\n"
     ]
    }
   ],
   "source": [
    "# Sorting relevancy rankings is the same between `df.TFIDF` and `df.Combined`\n",
    "df_sorted_old = df.sort_values('Combined', ascending=False)\n",
    "df_sorted_new = df.sort_values('TFIDF', ascending=False)\n",
    "assert np.array_equal(df_sorted_old['Word'].values,\n",
    "                      df_sorted_new['Word'].values)\n",
    "print(df_sorted_new[:10].to_string(index=False))"
   ]
  },
  {
   "source": [
    "Our word-rankings have remained unchanged. However, the values of the *TFIDF* and *Combined* columns are not identical. Our top 10 manually-computed Combined values are all greater than 1. Meanwhile, all of *Scikit-Learn's TFIDF* values are less than 1. Why is this the case?\n",
    "\n",
    "As it turns out, **Scikit-Learn automatically normalizes its TFIDF vector results**. The magnitude of `df.TFIDF` has been modified to equal 1. We can confirm by calling `norm(df.TFIDF.values)`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB. In order to turn off the normalization we must pass `norm=None` into the vectorizer’s initialization function. Running `TfidfVectorizer(norm=None, stop_words='english')` will return a vectorizer in which normalization has been deactivated"
   ]
  },
  {
   "source": [
    "SciKit-Learn has done this optimization ro more easily compute text-vector similarity when all vector magnitudes equal 1. Consequentlt, our normalized TFIDF matrix is primed for similarity analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmation that the TFIDF vector is normalized\n",
    "from numpy.linalg import norm\n",
    "assert norm(df.TFIDF.values) == 1, \"The TFIDF vector is not equal to 1.\""
   ]
  },
  {
   "source": [
    "\n",
    "\n",
    "### Common Scikit-Learn TfidfVectorizer Methods\n",
    "\n",
    "`tfidf_vectorizer = TfidfVectorizer(stopwords='english')`: Initializes a `TfidfVectorizer` object capable of vectorizing input texts based on their TFIDF values. The object is pre-set to filter common English stop words\n",
    "\n",
    "`tfidf_matrix = tfidf_vectorizer.fit_transform(texts)`: Executes TFIDF vectorization on a list of input texts, using the initialized vectorizer object. Returns CSR matrix of normalized TFIDF values. Each row of the matrix is automatically normalized, for easier similarity computation.\n",
    "\n",
    "`vocabulary_list = tifdf_vectorizer.get_feature_names()`: Returns the vocabulary-list associated with the columns of a computed TFIDF matrix. Each column `j` of the matrix corresponds to `vocabulary_list[j]`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}