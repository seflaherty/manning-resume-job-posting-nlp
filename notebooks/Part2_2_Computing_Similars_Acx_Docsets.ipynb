{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('job-posting-nlp': conda)",
   "display_name": "Python 3.7.6 64-bit ('job-posting-nlp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f811f0765f1ea4687b357010a64846d99dfd3ae554b01e3acc60a1b4eb302307"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Computing Similarities Across Large Document Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "excerpt from __Data Science Bookcamp: Five Python Projects__ MEAP V04 livebook by Leonard Apeltsin"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Sweeping parts of the explainer text in this notebook was taken from the liveProject notebook.\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Load the principal NumPy array\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "tfidf_np_matrix = load('../data/df_Words.npz')\n",
    "sample = tfidf_np_matrix['arr_0']\n",
    "print(sample[0])\n",
    "# print(sample.size) # -> its in the ballpark of 11,314 (nbr of posts) * 114,751 (nbr of unq words), but isn't exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.         0.00834093 0.04448717 ... 0.         0.00270615 0.01968562]\n"
     ]
    }
   ],
   "source": [
    "# Computing similarities to a single newsgrup post\n",
    "cosine_similarities = tfidf_np_matrix['arr_0'] @ tfidf_np_matrix['arr_0'][0]\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "source": [
    "The matrix-vector product took (ed. more than) a few seconds to complete and output a vector of cosine similarities. Each `i`th index of the vector corresponds to the cosine similarity between `newsgroup.data[0]` and `newsgroup.data[1]`. The printout shows that `cosine_similarities[0]` is equal to 1.0. This is *not surprising* sonce `newsgroups_data[0]` will have perfect similarity to itself. The next-highest similarity in the vector is found in `np.argsort(cosine_similarities)[-2]`. The call to `argsort` will sort the array indices by their ascending values. Thus, the second-to-last (i.e. that `[-2]`) index will correspond to the post with the second highest-similarity."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "NB. There is an assumption that no other posts exist with a perfect similarity of 1. There is also an alternative call you could do with `np.argmax(cosine_similarities[1:]) + 1` but it will only work for posts at index 0."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nThe following post has a cosine similarity of 0.64 with newsgroups.data[0]:\n\nIn article <1993Apr20.174246.14375@wam.umd.edu> lerxst@wam.umd.edu (where's my  \nthing) writes:\n> \n>  I was wondering if anyone out there could enlighten me on this car I saw\n> the other day. It was a 2-door sports car, looked to be from the late 60s/\n> early 70s. It was called a Bricklin. The doors were really small. In  \naddition,\n> the front bumper was separate from the rest of the body. This is \n> all I know. If anyone can tellme a model name, engine specs, years\n> of production, where this car is made, history, or whatever info you\n> have on this funky looking car, please e-mail.\n\nBricklins were manufactured in the 70s with engines from Ford. They are rather  \nodd looking with the encased front bumper. There aren't a lot of them around,  \nbut Hemmings (Motor News) ususally has ten or so listed. Basically, they are a  \nperformance Ford with new styling slapped on top.\n\n>    ---- brought to you by your neighborhood Lerxst ----\n\nRush fan?\n\n"
     ]
    }
   ],
   "source": [
    "# Deps\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(remove=('headers', 'footers'))\n",
    "\n",
    "# Finding the most similar newsgroup post\n",
    "most_similar_index = np.argsort(cosine_similarities)[-2]\n",
    "similarity = cosine_similarities[most_similar_index]\n",
    "most_similar_post = newsgroups.data[most_similar_index]\n",
    "print(f'---\\n\\nThe following post has a cosine similarity of {similarity:.2f} '\n",
    "       'with newsgroups.data[0]:\\n')\n",
    "print(most_similar_post)"
   ]
  },
  {
   "source": [
    "So we see that a reply contains the text of the original post (and we learn about the Bricklin in a world before web search). Due to the textual overlap, theie cosine similarity is 0.64. Although this number does not sound large,within an extensice text collections, a cosine similarity greater-than 0.6 is a good indicator of overlapping content."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "NB. The cosine similarity can easily be converted into the Tanimoto similarity. This is done by running `cosine_similarities / (2 - cosine_similarities)`. However, that conversion will not change the final result as the top index of the Tamimoto array is the same posted reply."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Realities: Why you should not reach to compute matrix of all-by-all cosine similaries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The TFIDF matrix has over 100k columns, so it is not computationally efficient to do this. We will reduce the column-count using Scikit-Learn's `TruncateSVD` class. \n",
    "\n",
    "Scikit-Learn's documentation occasioannly provides useful paramters for common algorithm application. The reduction of column count is available with the `n_components` parameter and the suggested value is 100 for processing text data. [RTFM](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nWe've dimensionally-reduced a 114441-column <class 'scipy.sparse.csr.csr_matrix'> matrix.\n---\n\nOur output is a 100-column <class 'numpy.ndarray'> matrix.\n"
     ]
    }
   ],
   "source": [
    "# Deps\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# Dimensionally reducing `tfidf_matrix` using SVD\n",
    "np.random.seed(0)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "shrunk_matrix = TruncatedSVD(n_components=100).fit_transform(tfidf_matrix)\n",
    "print(f'---\\n\\nWe\\'ve dimensionally-reduced a {tfidf_matrix.shape[1]}-column '\n",
    "      f'{type(tfidf_matrix)} matrix.\\n')\n",
    "print(f'---\\n\\nOur output is a {shrunk_matrix.shape[1]}-column '\n",
    "      f'{type(shrunk_matrix)} matrix.')"
   ]
  },
  {
   "source": [
    "Now shrunk, we can compute cosine similarities by running `shrunk_matrix @ shrunk_matrix.T`, but first we will confirm that the matrix rows remain normalized."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nThe magnitude of the first row is 0.49\n"
     ]
    }
   ],
   "source": [
    "# Deps\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Checking the magnitude of `shrunk_matrix[0]`\n",
    "magnitude = norm(shrunk_matrix[0])\n",
    "print(f'---\\n\\nThe magnitude of the first row is {magnitude:.2f}')"
   ]
  },
  {
   "source": [
    "ðŸ’¥ The magnitude is less-than 1, so the SVD output has not been automatically normalized. This will be done manually with built-in Scikit-Learn functions for our `shrunk_matrix`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nThe magnitude of the first row is 1.00\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the SVD output\n",
    "from sklearn.preprocessing import normalize\n",
    "shrunk_norm_matrix = normalize(shrunk_matrix)\n",
    "magnitude = norm(shrunk_norm_matrix[0])\n",
    "print(f'---\\n\\nThe magnitude of the first row is {magnitude:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the matrix of all-by-all cosine similarities\n",
    "cosine_similarity_matrix = shrunk_norm_matrix @ shrunk_norm_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---\n\nThe posts at indices 235 and 7805 share a cosine similarity of 0.91\n"
     ]
    }
   ],
   "source": [
    "# Leverage the new (from shrunken) cosine similarity matrix \n",
    "# from a random pair of similar posts\n",
    "np.random.seed(1) # nail down that seed\n",
    "index1 = np.random.randint(len(newsgroups.data))\n",
    "index2 = np.argsort(cosine_similarity_matrix[index1])[-2]\n",
    "similarity = cosine_similarity_matrix[index1][index2]\n",
    "print(f'---\\n\\nThe posts at indices {index1} and {index2} share a cosine '\n",
    "      f'similarity of {similarity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello,\n\tWho can tell me   Where can I find the PD or ShareWare   \nWhich can CAPTURE windows 3.1's output of printer mananger?\n\tI want to capture the output of HP Laser Jet III.\n\tThough the PostScript can setup to print to file,but HP can't.\n\tI try DOS's redirect program,but they can't work in Windows 3.1\n\t\tThankx for any help....\n--\n Internet Address: u7911093@cc.nctu.edu.tw\n    English Name: Erik Wang\n    Chinese Name: Wang Jyh-Shyang\n"
     ]
    }
   ],
   "source": [
    "# Printing the randomly chosen post\n",
    "print(newsgroups.data[index2].replace('\\n\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "u7911093@cc.nctu.edu.tw (\"By SWH ) writes:\n>Who can tell me which program (PD or ShareWare) can redirect windows 3.1's\n>output of printer manager to file? \n>\tI want to capture HP Laser Jet III's print output.\n> \tThough PostScript can setup print to file,but HP can't.\n>\tI use DOS's redirect program,but they can't work in windows.\n>\t\tThankx for any help...\n>--\n> Internet Address: u7911093@cc.nctu.edu.tw\n>    English Name: Erik Wang\n>    Chinese Name: Wang Jyh-Shyang\n> National Chiao-Tung University,Taiwan,R.O.C.\nTry setting up another HPIII printer but when choosing what port to connect it\nto choose FILE instead of like :LPT1.  This will prompt you for a file name\neverytime you print with that \"HPIII on FILE\" printer. Good Luck.\n\n"
     ]
    }
   ],
   "source": [
    "# Printing the most-similar response (my assumption is that it is an \"answer\")\n",
    "print(newsgroups.data[index1].replace('\\n\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}